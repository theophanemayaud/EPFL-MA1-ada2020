{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Homework 1 (HW1)\n",
    "\n",
    "---\n",
    "By the end of this homework we expect you to be able to:\n",
    "1. Load data from different formats using [pandas](https://pandas.pydata.org/);\n",
    "2. Navigate the documentation of Python packages by yourself;\n",
    "3. Filter and tidy up noisy data sets;\n",
    "4. Aggregate your data in different (and hopefully helpful) ways; and\n",
    "5. Create meaningful visualizations to analyze the data!\n",
    "---\n",
    "\n",
    "## Important Dates\n",
    "\n",
    "- Homework release: Fri 2 Oct 2020\n",
    "- **Homework due**: Fri 16 Oct 2020, 23:59\n",
    "- Grade release: Fri 23 Oct 2020\n",
    "\n",
    "---\n",
    "\n",
    "##  Some rules\n",
    "\n",
    "1. You are allowed to use any built-in Python library that comes with Anaconda. If you want to use an external library, you have to justify your choice.\n",
    "2. Make sure you use the `data` folder provided in the repository in **read-only** mode.\n",
    "3. Be sure to provide a textual description of your thought process, the assumptions you made, the solution you implemented, and explanations for your answers. A notebook that only has code cells will not suffice.\n",
    "4. For questions containing the **/Discuss:/** prefix, answer not with code, but with a textual explanation (in either comments or markdown).\n",
    "5. Back up any hypotheses and claims with data, since this is an important aspect of the course.\n",
    "6. Please write all your comments in English, and use meaningful variable names in your code. Your repo should have a single notebook (plus the required data files) in the master branch. If there are multiple notebooks present, we will not grade anything.\n",
    "7. Also, be sure to hand in a fully-run and evaluated notebook. We will not run your notebook for you, we will grade it as is, which means that only the results contained in your evaluated code cells will be considered, and we will not see the results in unevaluated code cells. In order to check whether everything looks as intended, you can check the rendered notebook on the GitHub website once you have pushed your solution there.\n",
    "8. Make sure to print results or dataframes that confirm you have properly addressed the task.\n",
    "\n",
    "\n",
    "\n",
    "## Context\n",
    "\n",
    "The coronavirus pandemic has led to the implementation of unprecedented non-pharmaceutical interventions ranging from case isolation to national lockdowns. These interventions, along with the disease itself, have created massive shifts in people’s lives. For instance, in mid-May 2020, more than one third of the global population was under lockdown, and millions have since lost their jobs or have moved to work-from-home arrangements.\n",
    "\n",
    "\n",
    "Importantly, the disease has shifted people's [needs](https://en.wikipedia.org/wiki/Toilet_paper), [interests](https://en.wikipedia.org/wiki/TikTok), and [concerns](https://en.wikipedia.org/wiki/COVID-19_pandemic_in_Switzerland) across the globe.\n",
    "\n",
    "In this homework, we will take a deep dive into Wikipedia data and try to uncover what changed with the pandemic. More specifically, we will be focusing on Wikipedia pageviews, that is, how many people read each article on Wikipedia each day.\n",
    "A nice graphical user interface for playing with Wikipedia pageviews is available [here](https://pageviews.toolforge.org/?project=en.wikipedia.org&platform=all-access&agent=user&redirects=0&range=latest-20&pages=Cat|Dog).\n",
    "Also, the Wikimedia Foundation releases dump files with the number of pageviews per article across all Wikimedia websites, including Wikipedia in all its language editions [(amazing, right?)](https://dumps.wikimedia.org/other/pagecounts-ez/). \n",
    "\n",
    "#### But wait, what is a pageview?\n",
    "\n",
    "> A pageview or page view, abbreviated in business to PV and occasionally called page impression, is a request to load a single HTML file (web page) of an Internet site. On the World Wide Web, a page request would result from a web surfer clicking on a link on another page pointing to the page in question. (Source: [Wikipedia article \"Pageviews\"](https://en.wikipedia.org/wiki/Pageview))\n",
    "\n",
    "Pageviews in Wikipedia can tell us that people are looking for certain information online. Analyzing how the volume and the distribution of pageviews changed can tell us about how the behavior of Wikipedia readers has changed.\n",
    "\n",
    "In this homework, you will take a deep dive into analyzing Wikipedia pageview logs and uncover shifts in interests associated with the current pandemic.\n",
    "\n",
    "---\n",
    "\n",
    "## The data\n",
    "\n",
    "First, you need to download a **meraviglioso** dataset from the Italian Wikipedia that we prepared for you. The structure of the data is described next. \n",
    "\n",
    "**The dataset is available in the `data` directory pushed to the same GitHub repo as the homework**. Inside of the data directory, you will find three files:\n",
    "\n",
    "### `articles.tsv.gz`\n",
    "\n",
    "This is a tab-separated file containing daily pageviews for a subset of the articles from Italian Wikipedia. It is compressed! Each row corresponds to a different article, and each column (except the first) corresponds to the number of pageviews this article received on a given day. The example below shows the structure for two of the things [Kristina Gligorić](https://kristinagligoric.github.io/), one of your TAs, likes the most on her Pizza:\n",
    "\n",
    "**Example:**\n",
    "~~~\n",
    "index       2018-01-01 00:00:00    2018-01-02 00:00:00 (...)\n",
    "Formaggio   100                    101                 (...)\n",
    "Ananas      12                     54                  (...)\n",
    "(...)       (...)                  (...)\n",
    "~~~\n",
    "\n",
    "\n",
    "### `topics.jsonl.gz`\n",
    "\n",
    "This is a classification of which topics an article belongs to, according to a model released by the Wikimedia Foundation (the classes are derived from this [taxonomy](https://www.mediawiki.org/wiki/ORES/Articletopic)). Importantly, this file was obtained from English Wikipedia, while the previous one contains articles from the Italian Wikipedia. This is important because article titles in the Italian Wikipedia are in Italian, while article titles in the English page are in English (duh!). In any case, each line contains a .json object with\n",
    "the English name of the article (name);\n",
    "1. the English name of the article (`name`);\n",
    "2. a set of fields related to topics themselves. Each of these fields is set as either `True` (if the article belongs to this topic) or `False` (if it does not). Notice that the same article may belong to multiple topics. \n",
    "\n",
    "**Example:**\n",
    "~~~\n",
    "{\"index\": \"Cheese\", \"Culture.Food and drink\": True, \"Culture.Literature\": False (...)}\n",
    "{\"index\": \"Pineapple\", \"Culture.Food and drink\": True, \"Culture.Literature\": False (...)}\n",
    "(...)\n",
    "~~~\n",
    "\n",
    " \n",
    "### `mapping.pickle`\n",
    "\n",
    "This is a `.pickle` file, that is, a serialized Python object. You can read about Python pickles  [here](https://wiki.python.org/moin/UsingPickle), \n",
    "but in short: the default Python library `pickle` allows you to save and load Python objects to and from disk. This is one object saved via the pickle library: a Python dictionary containing a mapping between the English names and the Italian names of Wikipedia articles:\n",
    "\n",
    "**Example:**\n",
    "~~~\n",
    "{\n",
    "    \"Cheese\": \"Formaggio\",\n",
    "    \"Ananas\": \"Pineapple\"\n",
    "    (...)\n",
    "}\n",
    "~~~\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## _Step 1:_ Loading the data\n",
    "\n",
    "---\n",
    "### **Task 1**\n",
    "\n",
    "Your first task is to load all these datasets into memory using pandas and pickle. \n",
    "**You should load the files compressed.**\n",
    "\n",
    "Here, the files at hand are rather small, and you could easily uncompress  the files to disk and work with them as plain text. \n",
    "Why, then, are we asking you to load the files compressed? The reason is that, in your life as a data scientist, this will often not be the case.\n",
    "\n",
    "Then, working with compressed files is key so that you don't receive e-mail from your (often more responsible) colleagues demanding to know how you have managed to fill the entire cluster with your datasets. \n",
    "Another big advantage of compressing files is to simply read files faster. You will often find that reading compressed data on the fly (uncompressing it as you go), is much faster than reading uncompressed data, since reading and writing to the disk may be your [bottleneck](https://skipperkongen.dk/2012/02/28/uncompressed-versus-compressed-read/). \n",
    "\n",
    " \n",
    "---\n",
    "\n",
    "**Hint:** `pandas` can open compressed files.\n",
    "\n",
    "**Hint:** In the real world (and in ADA-homework), your file often comes with some weird lines! \n",
    "This time you can safely ignore them (but in the real world you must try to understand why they are there!). Check the `error_bad_lines` parameter on `read_csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import pickle\n",
    "\n",
    "### ~ 1.1\n",
    "### Your code here! ###\n",
    "it_pageviews = pd.read_csv( './data/articles.tsv.gz', sep='\\t', error_bad_lines=False, index_col=\"index\")\n",
    "it_pageviews.columns = pd.to_datetime(it_pageviews.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#it_pageviews.index.is_unique\n",
    "#it_pageviews.T.index.is_unique\n",
    "#it_pageviews\n",
    "#it_pageviews.T.reset_index().plot.line(x=0, y='Tonino_Accolla')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_topics = pd.read_json('./data/topics.jsonl.gz', lines=True).set_index('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eng_topics.index = eng_topics['index']\n",
    "#eng_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_it_mapping = pd.read_pickle('./data/mapping.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eng_it_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## _Step 2:_ Filtering the data\n",
    "\n",
    "---\n",
    "### **Task 2**\n",
    "\n",
    "Oh no! Something seems wrong with your dataframe!\n",
    "It seems that some of the lines in the `articles.tsv.gz` are weird! \n",
    "They have titles in the format `\"Discussione:name_of_the_page\"`.\n",
    "\n",
    "Unsure of what they mean, you ask about them in the [Wiki-research mailing list](https://lists.wikimedia.org/mailman/listinfo/wiki-research-l).\n",
    "Twenty minutes later a kind internet stranger comes with an answer! \n",
    "She tells you that these are talk pages, where people discussing what should and should not be in the article (in fact it can be pretty funny to read, eg, [you can read Italians debating about pizza](https://it.wikipedia.org/wiki/Discussione:Pizza))\n",
    "\n",
    "After understanding what they are, your task is now to filter these lines using `pandas`! After all, we are interested in pageviews going towards articles! Not discussion pages!\n",
    "\n",
    "---\n",
    "\n",
    "**Hint**: There is one of them in the position \\#180 of the dataframe, if you want to check it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### ~ 2.1\n",
    "### Your code here! ###\n",
    "it_pageviews = it_pageviews[~it_pageviews.index.str.startswith('Discussione:')]\n",
    "it_pageviews[it_pageviews.index.str.startswith('Discussione:')]\n",
    "it_pageviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## *Step 3*: Understanding the data\n",
    "\n",
    "---\n",
    "### **Task 3.1**\n",
    "Data cleaning is hard huh? But now that this headache is behind us we can go on to explore our data.\n",
    "\n",
    "Let's begin with some basic stats. It is always important to do this as a sanity check.\n",
    "\n",
    "You should:\n",
    "\n",
    "1. Start by calculating how many topics and articles there are. Also, while you are at it, print the names of the topics to get a grasp of what they are about. \n",
    "2. Calculate the average daily number of pageviews in the dataset.\n",
    "3. **Discuss:** As previously mentioned, your data is a sample of _some_ (and not all) Wikipedia articles! Estimate (roughly) what percentage of Italian Wikipedia articles are in your dataset comparing your daily average pageview numbers with the official statistics (which can be found [here](https://pageviews.toolforge.org/siteviews/?platform=desktop&source=pageviews&agent=user&start=2020-01-01&end=2020-09-21&sites=it.wikipedia.org)). Notice that we are focusing on the desktop version of Wikipedia.\n",
    "\n",
    "---\n",
    "**Hint**: topics are in the columns of the topic file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### ~ 3.1.1\n",
    "### Your code here! ###\n",
    "print(f\"There are {it_pageviews.shape[0]} articles, and {eng_topics.shape[1]} topics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ~ 3.1.2\n",
    "### Your code here! ###\n",
    "print(f\"The average daily pageviews per page for the pages selected is {it_pageviews.mean(0).mean(0).round(2)}\")\n",
    "print(f\"The average daily pageviews cummulated for the all pages selected is {it_pageviews.mean(0).sum().round(2)}\")\n",
    "it_pageviews.mean(0).mean().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ~ 3.1.3\n",
    "### Your text (and code if necessary) here! ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "### **Task 3.2**\n",
    "\n",
    "Now that we have a better understanding of the data, let's look at some articles to get a feeling of what is happening. \n",
    "\n",
    "Your task is to:\n",
    "\n",
    "1. Find all articles whose names contain the sequence of characters `\"virus\"` (case insensitive) and that received least 7,000 pageviews across the entire period (no point in zooming on very insignificant articles);\n",
    "2. Find a way to nicely visualize __each__ one of the time-series (in a single plot, which may have multiple panels; in the lecture, Bob referred to these as “small multiples); Your visualization should allow one to see overall trends across each of the different articles and depict the overall trends, with the least noise possible. Additionally, highlight two specific dates in your plot: 31 January ([first case reported in Italy](https://en.wikipedia.org/wiki/COVID-19_pandemic_in_Italy#First_confirmed_cases)) and 21 February ([when multiple cases were confirmed in northern Italy](https://en.wikipedia.org/wiki/COVID-19_pandemic_in_Italy#Clusters_in_northern_Italy)).\n",
    "4. **Discuss**: What did you observe? Did all the articles behave similarly?\n",
    "\n",
    "---\n",
    "\n",
    "**Hint**: The column dates are currently strings which are not very plot friendly. You can turn them into datetime objects using: \n",
    "\n",
    "~~~python\n",
    "your_dataframe_name.columns = pd.to_datetime(your_dataframe_name.columns)\n",
    "~~~\n",
    "\n",
    "Notice that this only works if you have only date-related columns. Fortunately, if you get rid of the `index` column by making it a real pandas index, things should work just fine.\n",
    "\n",
    "**Hint**: Choose your axes wisely!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### ~ 3.2.1\n",
    "### Your code here! ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(14,3)) # change this if needed\n",
    "### ~ 3.2.2\n",
    "### Your code here! ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### ~ 3.2.3\n",
    "### Your text here! ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "### **Task 3.3**\n",
    "\n",
    "Before we move on, let's make a final sanity check and analyze the distribution of pageviews over all articles in our dataset. You are given a function to calculate the **cumulative distribution function** (CDF) of a sample. The CDF is a function f(x) associated with a data sample. For each value x, f(x) represents the percentage of elements in your sample that have values smaller or equal to x (read more about it [here](https://en.wikipedia.org/wiki/Empirical_distribution_function)).\n",
    "Your task is to:\n",
    "\n",
    "1. Calculate the CDF of the distribution of pageviews across all days over articles. That is, a) calculate the total number of pageviews each article has received and then, b) calculate the CDF for these values.\n",
    "\n",
    "\n",
    "2. Now plot this function using different scales for the x- and y-axis. You should plot it in 4 different ways:\n",
    "\n",
    "    a. x-axis on linear scale, y-axis on linear scale\n",
    "    \n",
    "    b. x-axis on log scale, y-axis on linear scale\n",
    "    \n",
    "    c. x-axis on linear scale, y-axis on log scale\n",
    "    \n",
    "    d. x-axis on log scale, y-axis on log scale\n",
    "    \n",
    "3. **Discuss:** There is a pretty odd fact about the distribution of our data! Can you spot it and describe it? Which of the different plots (a-d) allows you to find this oddity? Why isn't this visible in the other plots?\n",
    "\n",
    "---\n",
    "\n",
    "**Hint:** You can use `plt.xscale` and `plt.yscale`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This function is being given to you with a usage example :)! Make good use!\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_cdf(vals):\n",
    "    # Input:\n",
    "    # -- vals: an np.array of positive integers\n",
    "    # Output:\n",
    "    # -- x: an array containing all numbers from 1 to max(vals);\n",
    "    # -- y: an array containing the (empirically calculated) probability that vals <= x\n",
    "    \n",
    "    y = np.cumsum(np.bincount(vals)[1:])\n",
    "    y = y / y[-1]\n",
    "    y = y\n",
    "    x = list(range(1, max(vals) + 1))\n",
    "    return x, y  \n",
    "\n",
    "vals = np.array([1,2,3,4,1,2,4,3,4,4,5,4])\n",
    "x, y = get_cdf(vals)\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ~ 3.3.1\n",
    "### Your code here! ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ~ 3.3.2\n",
    "### Your code here! ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ~ 3.3.3\n",
    "### Your text here! ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## *Step 4*: Analyzing Overall Pageview Volume\n",
    "\n",
    "\n",
    "---\n",
    "### **Task 4.1**\n",
    "\n",
    "So far we have seen anecdotal examples. Now let’s move to the big picture! How did Wikipedia pageviews change in general? To gain a better understanding of how Wikipedia’s overall pageview volume has changed during the pandemic, you should do the following:\n",
    "\n",
    "1. Calculate and visualize the pageviews trends summed across **all** articles in Italian Wikipedia for the year 2020. (and only for 2020!). \n",
    "2. **Discuss**: what regular pattern (something that repeats over and over) do you see in the data?\n",
    "3. Pre-process the data to remove this regular pattern and make the overall trend clearer. Repeat the plot with the processed data.\n",
    "\n",
    "---\n",
    "\n",
    "**Hint**: A convenient way to use `.groupby` alongside dates is to use the [`pd.Grouper`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Grouper.html) class. Basically, it allows you to group by date periods given frequencies determined by the parameter `freq`. To read how to specify different types of frequencies, see [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases). Recall that, in order to turn an index, column index -- or pretty much anything -- into a timestamp, you can use  [`pd.to_timestamp`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_timestamp.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(14,3)) # change this if needed\n",
    "\n",
    "### ~ 4.1.1\n",
    "### Your code here! ###\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sum of all page views')\n",
    "plt.title('View trends in 2020 on our dataset')\n",
    "plt.plot( it_pageviews.T['2020'].T.columns, it_pageviews.T['2020'].sum(1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the column labels that are already in datetime format, with the sum of pageviews across pages for each day, and plot this. Numpy helps us to select the dates as we can specify simply the year and it returns all rows that are of that year (as we select rows, we must transpose the dataset first) ! We have imported the pyplot module as we it is useful to create simple plots with titles and labels to axes.\n",
    "From this first plot we see a specific pattern, it seems there are 4 to 5 spikes per month in searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ~ 4.1.2\n",
    "### Your text here! ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,3)) # change this if needed\n",
    "\n",
    "### ~ 4.1.3\n",
    "### Your code here! ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **Task 4.2**\n",
    "\n",
    "To get an even clearer picture, your task now is to compare the pageview time series of the current year (2020) with the time series of the previous year (2019).\n",
    "\n",
    "1. Make a visualization where the two years are somehow \"aligned\", that is, where it is possible to compare the same time of year across the two years. Additionally, your visualization should highlight the date on which the nationwide lockdown started in Italy, 9 March 2020. Preprocess each one of the time series (for each year) the same way you did in Task 4.1.\n",
    "\n",
    "3. **Discuss:** What changed from 2019 to 2020? Form and justify hypotheses about  the reasons behind this change.\n",
    "\n",
    "---\n",
    "\n",
    "**Hint**: In order to use two different y-axes in the same plot, you can use plt.twiny() or ax.twinx() (the latter if you are using the subplots environment;  [See this example](https://matplotlib.org/3.3.1/gallery/subplots_axes_and_figures/two_scales.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,3)) # change this if needed\n",
    "\n",
    "### ~ 4.2.1\n",
    "## Your code here! ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ~ 4.2.2\n",
    "### Your text here! ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## *Step 5*: Fiddling with Topics\n",
    "\n",
    "---\n",
    "### **Task 5.1**\n",
    "\n",
    "We now turn to a different question: what topics were impacted by the lockdown? \n",
    "To start unpacking this question, your task now is to aggregate, for each day, all pageviews that went to each one of the 64 topics. \n",
    "\n",
    "There are multiple ways to do this, but for the sake of this exercise, you must create a dataframe where each row contains the number of pageviews a topic obtained on a given day! Example:\n",
    "\n",
    "~~~\n",
    "index       date                   views             \n",
    "TOPIC1      2019-01-01             101              \n",
    "TOPIC1      2019-01-02             151             \n",
    "(...)       (...)                  (...)\n",
    "TOPICK      2019-01-01             1010              \n",
    "TOPICK      2019-01-02             2123            \n",
    "(...)       (...)                  (...)\n",
    "~~~\n",
    "\n",
    "---\n",
    "\n",
    "**Hint**: You've should find a way to make the index in the dataframe with the topics be the same as the index in the dataframes with the articles. See the file `mapping.pickle`.\n",
    "\n",
    "**Hint**: You may want to use `.melt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### ~ 5.1.1\n",
    "### Your code here! ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **Task 5.2**\n",
    "\n",
    "Now to the **grand finale**. We will consider two periods:\n",
    "- the 35 days before the quarantine started (in the 9th of March); and \n",
    "- the 34 days after the quarantined started (including the day of the quarantine itself).\n",
    "\n",
    "Create a visualization where you can compare, for each topic, the mean **number of views** in the aforementioned periods (that is, before and after the quarantine started). **Although there is a very large number of topics, your visualization should be a compact panel, small enough to fit on an A4 page.**\n",
    "\n",
    "---\n",
    "\n",
    "**Hint**: [Hoooray](https://seaborn.pydata.org/examples/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(14,10)) # change this if needed\n",
    "\n",
    "### ~ 5.2\n",
    "### Your code here! ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **Task 5.3**\n",
    "\n",
    "Notice that the previous analysis fails to isolate the increases or decreases in each individual topic from the overall increases or decreases in pageviews across Wikipedia in general. That is, it could be that all topics gained/lost pageviews, but some did so more than articles in general, while others did so less than articles in general. To address this issue, you should:\n",
    "\n",
    "\n",
    "1. Normalize the pageviews counts in the dataframe created in Task 5.1. Instead of using the raw number of pageviews, you should compute, for each day, what fraction of all pageviews a topic received.\n",
    "\n",
    "2. Create a second visualization that shows not the **raw** value of pageviews before and after, but the **relative** value that you just calculated.\n",
    "\n",
    "3. **Discuss:** According to Task 5.2, what topics have increased in terms of the raw, absolute number of pageviews after the quarantine started? In relative, rather than absolute, terms, do these findings still hold? If not, what has changed?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ~ 5.3.1\n",
    "### Your code here! ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ~ 5.3.2\n",
    "### Your code here! ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ~ 5.3.3\n",
    "### Your text here! ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
